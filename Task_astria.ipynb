{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMneCp2oEIqYSuGAIBjQ9Pw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjitben10/Rag_examples/blob/main/Task_astria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community pypdf google-generativeai langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7SIS79uQl6a",
        "outputId": "d261c1d1-7116-4d69-f8fc-d83e333077ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n",
            "  Downloading langchain_core-0.3.14-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.65.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain_community-0.3.4-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.6-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.3-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.14-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain_google_genai, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.4\n",
            "    Uninstalling langchain-0.3.4:\n",
            "      Successfully uninstalled langchain-0.3.4\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.6 langchain-core-0.3.14 langchain_community-0.3.4 langchain_google_genai-2.0.3 marshmallow-3.23.0 mypy-extensions-1.0.0 pydantic-settings-2.6.0 pypdf-5.1.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "llm = GoogleGenerativeAI(model=\"gemini-pro\")"
      ],
      "metadata": {
        "id": "UnxiBjIeSOnV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_SBuYhb4YTJ",
        "outputId": "35a3269d-5591-47a6-9b73-356b3e82da0a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_ollama\n",
            "  Downloading langchain_ollama-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_ollama) (0.3.14)\n",
            "Collecting ollama<1,>=0.3.0 (from langchain_ollama)\n",
            "  Downloading ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (4.12.2)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from ollama<1,>=0.3.0->langchain_ollama) (0.27.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (3.10.10)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (1.2.2)\n",
            "Downloading langchain_ollama-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading ollama-0.3.3-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: ollama, langchain_ollama\n",
            "Successfully installed langchain_ollama-0.2.0 ollama-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "model = OllamaLLM(model=\"llama3.1\")\n"
      ],
      "metadata": {
        "id": "4f0pugh24Wcy"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yYGOk1PVpNR",
        "outputId": "2100e640-08e9-4926-8464-956fffeaef25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, DataFrameLoader\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "GOOGLE_API_KEY = \"AIzaSyAziDIq4oAKaZZV45Jg-6aub8sNEn\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-pro\")"
      ],
      "metadata": {
        "id": "DwtNGkugTSvd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import Docx2txtLoader"
      ],
      "metadata": {
        "id": "BReOixakV251"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-community unstructured openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wir5vt96WsPJ",
        "outputId": "f777ab16-ab97-46c2-8921-0dc502d6bcdc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.4/981.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredExcelLoader"
      ],
      "metadata": {
        "id": "RaOR1FtSWuYw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load PDF document\n",
        "pdf_loader = PyPDFLoader(\"/content/RFP1.pdf\")\n",
        "pdf_documents = pdf_loader.load()\n",
        "\n",
        "# Load Excel document\n",
        "excel_loader = UnstructuredExcelLoader(\"/content/RFP2.xlsx\", mode=\"elements\")\n",
        "excel_documents = excel_loader.load()\n",
        "\n",
        "# Load Word document\n",
        "docx_loader = Docx2txtLoader(\"/content/RFP3.docx\")\n",
        "docx_documents = docx_loader.load()\n",
        "\n",
        "# Combine all documents into a single list\n",
        "all_documents = pdf_documents + excel_documents + docx_documents\n",
        "\n",
        "# Add metadata to differentiate sources (optional but helpful)\n",
        "for doc in pdf_documents:\n",
        "    doc.metadata[\"source\"] = \"pdf\"\n",
        "for doc in excel_documents:\n",
        "    doc.metadata[\"source\"] = \"excel\"\n",
        "for doc in docx_documents:\n",
        "    doc.metadata[\"source\"] = \"docx\"\n"
      ],
      "metadata": {
        "id": "Om9_LewBURIx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24aD6QOwXNzn",
        "outputId": "a9ae6582-4cd0-4c06-8ae1-9950fed9a84e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"\"\"\n",
        "You are an Information Extractor tasked with identifying specific event-related details from the provided document content. Carefully interpret and extract relevant information based on context, and include each field as part of your response if it is available.\n",
        "\n",
        "### Instructions:\n",
        "1. For each event, identify details such as date, day, start time, end time, function type, setup style, people count, and any additional comments.\n",
        "2. If any information is missing in the document, simply omit it in the response.\n",
        "3. Ensure that each response contains only the extracted details for each event, listing each available piece of information without enforcing empty fields.\n",
        "\n",
        "### Desired Response Format:\n",
        "- Provide each event as a collection of extracted fields with details such as:\n",
        "  - **Date**: Extracted as MM/DD/YYYY if available.\n",
        "  - **Day**: The day of the week associated with the event, if mentioned.\n",
        "  - **Repeat Count**: Number of repetitions if specified.\n",
        "  - **Start Time**: Start time for the event.\n",
        "  - **End Time**: End time for the event.\n",
        "  - **Function Type**: Type of function or activity, like \"Office\", \"Storage\", etc.\n",
        "  - **Setup Style**: Setup style or layout for the event.\n",
        "  - **People Count**: Expected number of attendees if mentioned.\n",
        "  - **Comments**: Additional notes or relevant details.\n",
        "\n",
        "Document Content:\n",
        "{context}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "QNnHGBtESmUY"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt_text=\"\"\"\n",
        "# You are an advanced Information Extractor tasked with identifying specific event-related details from the provided document content. Carefully interpret and map each piece of information to the correct field based on context, prioritizing only relevant information for each key.\n",
        "\n",
        "# ### Instructions:\n",
        "# 1. **Identify Key Information**: For each event, identify and classify details such as date, day, start time, end time, function type, setup style, people count, and additional comments.\n",
        "#    - **Focus on Recognizable Cues**: Use language cues in the document, such as \"day,\" \"date,\" \"people,\" or \"type,\" to help categorize information accurately.\n",
        "# 2. **Map to Fields Dynamically**: Each field should contain only the information relevant to it, avoiding unnecessary details or placeholder text (like \"N/A\") for missing information.\n",
        "#    - Omit fields where the document provides no relevant content.\n",
        "# 3. **Include Contextual Keywords**: Pay attention to contextual words that indicate function type (e.g., \"meeting,\" \"conference\"), setup style (e.g., \"banquet,\" \"theater\"), or attendee count.\n",
        "# 4. **Avoid Redundant Fields**: If details are repeated or implied by other fields (e.g., \"arrival day\" suggests \"day\" as \"Sunday\"), consolidate these logically within one field.\n",
        "\n",
        "# ### Desired Response Format:\n",
        "# - For each event, include only relevant extracted fields with details such as:\n",
        "#   - **Date**: Provide as MM/DD/YYYY if available; otherwise, use descriptive timing like \"Mid-February\" where precise dates are absent.\n",
        "#   - **Day**: The specific day of the week (e.g., \"Monday\") or day range (e.g., \"Saturday - Wednesday\") as mentioned.\n",
        "#   - **Repeat Count**: Include only if the document specifies a repeat count or recurrence.\n",
        "#   - **Start Time** and **End Time**: Times provided for the start and end of the event.\n",
        "#   - **Function Type**: Type of function (e.g., \"Annual Sales Meeting,\" \"Vendor Tradeshow\") if provided.\n",
        "#   - **Setup Style**: Event setup style (e.g., \"classroom,\" \"banquet\").\n",
        "#   - **People Count**: Expected number of attendees; map any mention of numbers related to people here.\n",
        "#   - **Comments**: Additional details, instructions, or notes relevant to the event setup or requirements.\n",
        "\n",
        "# For each event, present only the fields containing relevant extracted information.\n",
        "\n",
        "# Document Content:\n",
        "# {context}\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "7ogi6PSBUO3G"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt_text = \"\"\"\n",
        "# You are an Information Extractor specialized in event planning details. Your task is to analyze the provided document content and accurately categorize details into specific fields, using both keywords and contextual understanding to guide your choices. Focus on identifying the most relevant information for each field, avoiding any placeholders or redundant entries.\n",
        "\n",
        "# ### Instructions for Field Extraction:\n",
        "# 1. **Identify Relevant Information**: For each event, determine key details such as date, day, start time, end time, function type, setup style, people count, and comments.\n",
        "#    - **Use Content Cues**: Identify phrases, keywords, or patterns that indicate specific fields (e.g., \"Saturday - Wednesday\" as **Day**, \"2800 people\" as **People Count**).\n",
        "#    - **Filter Out Non-relevant Data**: Only include information directly related to event details; ignore unrelated text.\n",
        "\n",
        "# 2. **Map to Appropriate Fields**:\n",
        "#    - Include each field only if it is present in the document.\n",
        "#    - Avoid placeholders (like \"N/A\") for missing data.\n",
        "\n",
        "# 3. **Field-specific Guidance**:\n",
        "#    - **Date**: Look for exact dates (e.g., \"02/15/2013\") or timeframes (e.g., \"Mid-February\") and provide as **Date**.\n",
        "#    - **Day**: Extract the day or range of days if specified (e.g., \"Monday\" or \"Saturday - Wednesday\").\n",
        "#    - **Repeat Count**: Include only if a repetition or recurrence is mentioned (e.g., \"repeats weekly\").\n",
        "#    - **Start Time / End Time**: Include event start and end times if they are given in hours or time expressions.\n",
        "#    - **Function Type**: Identify event types like \"Sales Meeting,\" \"Tradeshow,\" or similar descriptors that indicate the purpose or nature of the event.\n",
        "#    - **Setup Style**: Note the event’s layout setup, such as \"banquet,\" \"classroom,\" or \"theater style\" if mentioned.\n",
        "#    - **People Count**: Map any specific number of attendees to **People Count** (e.g., \"2800 people\").\n",
        "#    - **Comments**: Include any special notes, instructions, or specific requirements related to the event setup, such as \"complimentary internet at Registration Desk.\"\n",
        "\n",
        "# ### Response Format:\n",
        "# - For each event, list only the fields that contain relevant information in the following format:\n",
        "#   - **Date**: [Event date, if provided]\n",
        "#   - **Day**: [Day of the event, if provided]\n",
        "#   - **Repeat Count**: [Recurrence details, if mentioned]\n",
        "#   - **Start Time**: [Event start time, if available]\n",
        "#   - **End Time**: [Event end time, if available]\n",
        "#   - **Function Type**: [Event function type, if provided]\n",
        "#   - **Setup Style**: [Event setup style, if available]\n",
        "#   - **People Count**: [Attendee count, if available]\n",
        "#   - **Comments**: [Additional notes or requirements, if provided]\n",
        "\n",
        "# Document Content:\n",
        "# {context}\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "MMDvUa5vVlA-"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Sample output string (replace this with your actual output)\n",
        "response = \"\"\"\n",
        "- **Date**: N/A\n",
        "- **Day**: Sunday\n",
        "- **Repeat Count**: N/A\n",
        "- **Start Time**: N/A\n",
        "- **End Time**: N/A\n",
        "- **Function Type**: Annual Sales Meeting\n",
        "- **Setup Style**: N/A\n",
        "- **People Count**: 1890\n",
        "- **Comments**: Main Arrivals\n",
        "\n",
        "- **Date**: N/A\n",
        "- **Day**: Monday\n",
        "- **Repeat Count**: N/A\n",
        "- **Start Time**: N/A\n",
        "- **End Time**: N/A\n",
        "- **Function Type**: Annual Sales Meeting\n",
        "- **Setup Style**: N/A\n",
        "- **People Count**: 1890\n",
        "- **Comments**: N/A\n",
        "\"\"\"\n",
        "\n",
        "# Split the response into individual event strings\n",
        "event_strings = re.split(r'(?=- \\*\\*Date\\*\\*:)', response.strip())\n",
        "\n",
        "# Create a list to hold event dictionaries\n",
        "events = []\n",
        "\n",
        "# Process each event string to extract details\n",
        "for event_str in event_strings:\n",
        "    event_dict = {}\n",
        "\n",
        "    # Extract each field using regex\n",
        "    for line in event_str.splitlines():\n",
        "        if \": \" in line:\n",
        "            key, value = line.split(\": \", 1)  # Split on first occurrence\n",
        "            # Clean up the key and value\n",
        "            key = key.strip().replace(\"- **\", \"\").replace(\"**\", \"\")\n",
        "            value = value.strip()\n",
        "            event_dict[key] = value\n",
        "\n",
        "    # Append the event dictionary to the events list if it's not empty\n",
        "    if event_dict:\n",
        "        events.append(event_dict)\n",
        "\n",
        "# Write the events list to a JSON file\n",
        "with open('events_output.json', 'w') as json_file:\n",
        "    json.dump({\"events\": events}, json_file, indent=4)\n",
        "\n",
        "print(\"Events have been written to events_output.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSjNR-lUPXsd",
        "outputId": "abc08c73-85bb-45a4-ee13-dfd4b08002d0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Events have been written to events_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBLKvMI6PiaI",
        "outputId": "41751b2f-31c9-42b6-d785-e9e52d9f7a5c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Date': 'N/A',\n",
              "  'Day': 'Sunday',\n",
              "  'Repeat Count': 'N/A',\n",
              "  'Start Time': 'N/A',\n",
              "  'End Time': 'N/A',\n",
              "  'Function Type': 'Annual Sales Meeting',\n",
              "  'Setup Style': 'N/A',\n",
              "  'People Count': '1890',\n",
              "  'Comments': 'Main Arrivals'},\n",
              " {'Date': 'N/A',\n",
              "  'Day': 'Monday',\n",
              "  'Repeat Count': 'N/A',\n",
              "  'Start Time': 'N/A',\n",
              "  'End Time': 'N/A',\n",
              "  'Function Type': 'Annual Sales Meeting',\n",
              "  'Setup Style': 'N/A',\n",
              "  'People Count': '1890',\n",
              "  'Comments': 'N/A'}]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import json,re\n",
        "# Set up the prompt template and LLMChain\n",
        "prompt_template = PromptTemplate(input_variables=[\"context\"], template=prompt_text)\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "events=[]\n",
        "res=\"\"\n",
        "# Run the chain for each document individually\n",
        "for doc in all_documents:\n",
        "    document_content = doc.page_content\n",
        "    #print(document_content)\n",
        "    output = llm_chain.run({\"context\": document_content})\n",
        "    events.append({\"content\": output})\n",
        "    res+=output\n",
        "\n",
        "# Split the response into individual event strings\n",
        "event_strings = re.split(r'(?=- \\*\\*Date\\*\\*:)', res.strip())\n",
        "\n",
        "# Create a list to hold event dictionaries\n",
        "events = []\n",
        "\n",
        "# Process each event string to extract details\n",
        "for event_str in event_strings:\n",
        "    event_dict = {}\n",
        "\n",
        "    # Extract each field using regex\n",
        "    for line in event_str.splitlines():\n",
        "        if \": \" in line:\n",
        "            key, value = line.split(\": \", 1)  # Split on first occurrence\n",
        "            # Clean up the key and value\n",
        "            key = key.strip().replace(\"- **\", \"\").replace(\"**\", \"\")\n",
        "            value = value.strip()\n",
        "            event_dict[key] = value\n",
        "\n",
        "    # Append the event dictionary to the events list if it's not empty\n",
        "    if event_dict:\n",
        "        events.append(event_dict)\n",
        "\n",
        "# Write the events list to a JSON file\n",
        "with open('events_output.json', 'w') as json_file:\n",
        "    json.dump({\"events\": events}, json_file, indent=4)\n",
        "\n",
        "print(\"Events have been written to events_output.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7xsjBTBmSVFR",
        "outputId": "baa6c52b-7dad-48b7-f6d9-9a800d656c63"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Events have been written to events_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DvXQ-F9SWIf",
        "outputId": "9941bf53-bcc3-48f5-f9f3-1619823e76c6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Date': '07/30/2023 - 08/02/2023',\n",
              " 'Day': 'Friday - Monday',\n",
              " 'Repeat Count': '3',\n",
              " 'Start Time': '08:00 AM',\n",
              " 'End Time': '24 hour - Wed. noon',\n",
              " 'Function Type': 'General Session / Ballroom',\n",
              " 'Setup Style': 'Rounds',\n",
              " 'People Count': '2,500',\n",
              " 'Comments': '60,000 sf.'}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mbROF3QSXZc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}